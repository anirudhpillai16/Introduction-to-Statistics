p.girl = girls / (3*families)
# Now do chi-squared test
observed = c(23236, 58529, 53908, 18770)
expected = c(families*dbinom(0, 3, p.girl),
families*dbinom(1, 3, p.girl),
families*dbinom(2, 3, p.girl),
families*dbinom(3, 3, p.girl))
# Shorter code:
expected = families * dbinom(0:3, 3, p.girl)
# Pearson's X^2
X2 = sum((observed - expected)^2/expected)
1 - pchisq(X2, df=2)
observed - expected
# Likelihood ratio
G2 = 2 * sum(observed * log(observed/expected))
1 - pchisq(G2, df=2)
### Premier League goals, 2000-01
# Observed number of games with 0 to 8 goals
observed = c(28, 67, 100, 88, 52, 25, 13, 6, 1)
plot(0:8, observed, type="h")
# Maybe Poisson?
games = sum(observed)
goals = sum((0:8)*observed)
ave = goals/games
expected = games * dpois(0:20, ave)
expected
round(expected, 1)
# Try again
observed = c(28, 67, 100, 88, 52, 25, 13, 7)
expected = rep(NA, 8)
expected[1:7] = games * dpois(0:6, ave)
expected[8] = games * (1 - ppois(6, ave))
sum(expected)
# Do test
X2 = sum((observed - expected)^2/expected)
1 - pchisq(X2, df=6)
### Handedness by sex
# Right-handed: 934 men, 1070 women
# Left-handed: 113 men, 92 women
# Ambidextrous: 20 men, 8 women
observed = c(934, 1070, 113, 92, 20, 8)
N = sum(observed)
RH = (934 + 1070) / N
LH = (113 + 92) / N
ambi = (20 + 8) / N
men = (934 + 113 + 20) / N
women = c(1070 + 92 + 8) / N
expected = c(RH*men*N, RH*women*N, LH*men*N,
LH*women*N, ambi*men*N, ambi*women*N)
X2 = sum((observed - expected)^2/expected)
1 - pchisq(X2, df=2)
# 20,000 die rolls
observed = c(3407, 3631, 3176, 2916, 3448, 3422)
# H0: Die is fair
expected = rep(20000/6, 6)
# LR chi-squared test
G2 = 2 * sum(observed * log(observed/expected))
1 - pchisq(G2, df=5)
# Pearson's chi-squared
X2 = sum((observed - expected)^2 / expected)
1 - pchisq(X2, df=5)
# Observe the genders of the first three children
# in families with 3 or more kids (in Denmark)
# 0 girls: 23236
# 1 girl: 58529
# 2 girls: 53908
# 3 girls: 18770
# Does the number of girls follow a binomial?
# Treat as random process
# First: Estimate the proportion of girls
families = 18770 + 53908 + 58529 + 23236
girls = 3*18770 + 2*53908 + 58529
p.girl = girls / (3*families)
# Now do chi-squared test
observed = c(23236, 58529, 53908, 18770)
expected = c(families*dbinom(0, 3, p.girl),
families*dbinom(1, 3, p.girl),
families*dbinom(2, 3, p.girl),
families*dbinom(3, 3, p.girl))
# Shorter code:
expected = families * dbinom(0:3, 3, p.girl)
# Pearson's X^2
X2 = sum((observed - expected)^2/expected)
1 - pchisq(X2, df=2)
observed - expected
# Likelihood ratio
G2 = 2 * sum(observed * log(observed/expected))
1 - pchisq(G2, df=2)
### Premier League goals, 2000-01
# Observed number of games with 0 to 8 goals
observed = c(28, 67, 100, 88, 52, 25, 13, 6, 1)
plot(0:8, observed, type="h")
# Maybe Poisson?
games = sum(observed)
goals = sum((0:8)*observed)
ave = goals/games
expected = games * dpois(0:20, ave)
expected
round(expected, 1)
# Try again
observed = c(28, 67, 100, 88, 52, 25, 13, 7)
expected = rep(NA, 8)
expected[1:7] = games * dpois(0:6, ave)
expected[8] = games * (1 - ppois(6, ave))
sum(expected)
# Do test
X2 = sum((observed - expected)^2/expected)
1 - pchisq(X2, df=6)
### Handedness by sex
# Right-handed: 934 men, 1070 women
# Left-handed: 113 men, 92 women
# Ambidextrous: 20 men, 8 women
observed = c(934, 1070, 113, 92, 20, 8)
N = sum(observed)
RH = (934 + 1070) / N
LH = (113 + 92) / N
ambi = (20 + 8) / N
men = (934 + 113 + 20) / N
women = c(1070 + 92 + 8) / N
expected = c(RH*men*N, RH*women*N, LH*men*N,
LH*women*N, ambi*men*N, ambi*women*N)
X2 = sum((observed - expected)^2/expected)
1 - pchisq(X2, df=2)
observed = c(3407, 3631, 3176, 2916, 3448, 3422)
# H0: Die is fair
expected = rep(20000/6, 6)
rep??
??rep
?rep
??rep
?na
?na
?NA
observed<- c(29,19,18,25,17,10,15,11)
# Ho:- Horse's starting position does not affect winning
expected<-rep(144/8, 8)
G2 = 2 * sum(observed * log(observed/expected))
1 - pchisq(G2, df=7)
X2 = sum((observed - expected)^2 / expected)
1 - pchisq(X2, df=7)
count<- c(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14)
freq<- c(57,203,383,525,532,408,273,139,45,27,10,4,0,1,1)
x_bar=sum(count*freq)
x_bar=sum(count*freq)/sum(freq)
dpois(count[1:9],x_bar[1:9])
expected=dpois(count,x_bar)
expected=dpois(count,x_bar)*sum(freq)
expected
1-ppois(9,x_bar)*2608
exp=1-ppois(9,x_bar)*2608
for(i in 1:9)
expected=dpois(count,x_bar)*sum(freq)
exp=1-ppois(9,x_bar)*2608
count<- c(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14)
freq<- c(57,203,383,525,532,408,273,139,45,27,10,4,0,1,1)
# a) Average Observed Count
x_bar=sum(count*freq)/sum(freq)
x_bar
# b)
for(i in 1:9)
expected=dpois(count,x_bar)*sum(freq)
expected[i]=dpois(count[i],x_bar[i])*2608
expected[i]=dpois(count[i],x_bar[i])*2608
expected[i]
for(i in 1:9)
{
expected[i]=dpois(count[i],x_bar[i])*2608
expected[i]
count<- c(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14)
freq<- c(57,203,383,525,532,408,273,139,45,27,10,4,0,1,1)
# a) Average Observed Count
x_bar=sum(count*freq)/sum(freq)
x_bar
# b)
for(i in 1:9)
{
expected[i]=dpois(count[i],x_bar[i])*2608
expected[i]
}
# See also the html file for the regression lecture
batting = read.table("Teaching/batting.txt", header=TRUE)
batting = read.table("C:/Stats/batting.txt", header=TRUE)
summary(batting)
plot(batting$OB2012, batting$OB2013)
# Model 1: Predict the mean
# for everyone
mean(batting$OB2013)
SST = sum((batting$OB2013 - mean(batting$OB2013))^2)
abline(h=mean(batting$OB2013), col="blue")
# Model 2: Predict using regression
slope = cor(batting$OB2012, batting$OB2013) *
sd(batting$OB2013) / sd(batting$OB2012)
intercept = mean(batting$OB2013) -
slope * mean(batting$OB2012)
predictions = intercept + slope*batting$OB2012
SSE = sum((batting$OB2013 - predictions)^2)
abline(intercept, slope, col="red")
# Ratio
SSE/SST
1 - cor(batting$OB2012, batting$OB2013)^2
# Model 3: Predict 2013 OB = 2012 OB
abline(0, 1, col="orange")
sum((batting$OB2013 - batting$OB2012)^2)
# Inference for the slope
# Test hyp that slope = 0
se = sd(batting$OB2013)/sd(batting$OB2012) *
sqrt((1-cor(batting$OB2012, batting$OB2013)^2)/
(length(batting$OB2012)-2))
T = slope / se
# P-value
2 * (1-pt(T, length(batting$OB2012)-2))
# 95% CI
slope - qt(.975, df=length(batting$OB2012)-2)*se
slope + qt(.975, df=length(batting$OB2012)-2)*se
# F-test
r = cor(batting$OB2012, batting$OB2013)
F = (length(batting$OB2012)-2)*r^2/(1-r^2)
# P-value
1 - pf(F, 1, length(batting$OB2012)-2)
# Prediction
# Robinson Cano:
# 2012: 264 OB, 697 AB
# 2013: ? OB, 681 AB
qqnorm(batting$OB2012)
qqnorm(batting$OB2013)
residuals = batting$OB2013 - predictions
qqnorm(residuals)
# Create two new ratio variables
obp2012 = batting$OB2012 / batting$PA2012
obp2013 = batting$OB2013 / batting$PA2013
qqnorm(obp2012)
qqnorm(obp2013)
plot(obp2012, obp2013)
# Fit regression
slope = cor(obp2012,obp2013)*sd(obp2013)/
sd(obp2012)
intercept = mean(obp2013)-slope*mean(obp2012)
# Check
lm(obp2013 ~ obp2012)
cano2012 = 264/697
# Point prediction for 2013 OBP
slope * cano2012 + intercept
# Point prediction for 2013 OB
(slope * cano2012 + intercept) * 681
# Prediction error
resOBP = obp2013 - (slope*obp2012+intercept)
SSE.OBP = sum(resOBP^2)
pred.error = sqrt(SSE.OBP/(length(obp2012)-2))
pred.error * 681
sister<- c(69,64,65,63,65,62,65,64,66,59,62)
brother<- c(71,68,66,67,70,71,70,73,72,65,66)
mean(brother)
mb=mean(brother)
ms=mean(sister)
n=length(sister)
sds=sd(sister)
sds=cor(sister)
sds=cor(sister,brother)
sx=sum(sister-ms)/(n-1)
sx=(sum(sister-ms))/(n-1)
sx=(sum(sister-ms)^2)/(n-1)
sx=(sum(sister-ms)^2)
sx=sum((sister-ms)^2)
sx=sum((sister-ms)^2)/n-1
sx=sum((sister-ms)^2)/(n-1)
sy=sum((brother-mb)^2)/(n-1)
r=cor(sister,brother)
sister<- c(69,64,65,63,65,62,65,64,66,59,62)
brother<- c(71,68,66,67,70,71,70,73,72,65,66)
mb=mean(brother)
ms=mean(sister)
n=length(sister)
sx=sum((sister-ms)^2)/(n-1)
sy=sum((brother-mb)^2)/(n-1)
r2=cor(sister,brother)
r=sqrt(r2)
r=cor(sister,brother)
r2=r^2
all<- c(sister,brother)
mall=mean(all)
SST = sum( (all-mall)^2 )/21
SST = sy*(n-1)
SSB = n*(sister-ms)^2 + n*(brother-mb)^2
bss=SSB/df1
df1=1
bss=SSB/df1
SSB = n*(ms-mall)^2 + n*(mb-mall)^2
df1=1
bss=SSB/df1
SSB = n*(ms-mall)^2 + n*(mb-mall)^2
SSB = r^2 *SST
SSR = r^2 *SST
SSR = r2 *SST
SSE=(1-r2)*SST
df1=1
df2=9
msr=r2*SST
mse=SSE/df2
f=(n-2)*r2/(1-r2)
1-pf(f,df1,df2)
beta1=r*(sy/sx)
syy=sqrt(sy)
sxx=sqrt(sx)
beta1=r*(syy/sxx)
gt=qt(0.95,df2)
se=((1-r2)*sy)/(n-2)*sx
se=((1-r2)*sy)/((n-2)*sx)
print("Confidence Interval is=")
lower=beta1- (gt*se)
lower=beta1- (gt*sqrt(se))
upper=beta1+ (gt*sqrt(se))
lower
upper
qt(0.975,df=9)
x<- c(-0.2,-0.9,-0.4,0.6,0.4)
y<- c(0.4,-0.3,-0.3,0.5,1.1)
r=cor(x,y)
x<- c(-0.2,-0.9,-0.4,0.6,0.4)
y<- c(0.4,-0.3,-0.3,0.5,1.1)
# a) Pearson's Correlation Coefficient is given by
print("Pearson's Correlation Coefficient is =")
r=cor(x,y)
r
plot(x,y)
plot(y,x)
plot(x,y)
slope = r *(sd(y) / sd(x))
intercept = mean(y) - (slope * mean(x))
predictions = intercept + slope*x
predictions = intercept + (slope*x)
predictions = intercept + slope*x
SSE = sum((y - predictions)^2)
abline(intercept, slope, col="red")
plot(y,x)
slope = r *(sd(x) / sd(y))
intercept = mean(x) - (slope * mean(y))
predictions = intercept + slope*x
SSE = sum((x - predictions)^2)
abline(intercept, slope, col="blue")
r2=cor(y,x)
slope = r2 *(sd(x) / sd(y))
intercept = mean(x) - (slope * mean(y))
predictions = intercept + slope*x
SSE = sum((x - predictions)^2)
x<- c(-0.2,-0.9,-0.4,0.6,0.4)
y<- c(0.4,-0.3,-0.3,0.5,1.1)
# a) Pearson's Correlation Coefficient is given by
print("Pearson's Correlation Coefficient is =")
r=cor(x,y)
r
# b)
plot(x,y)
# Predict y from x
slope = r *(sd(y) / sd(x))
intercept = mean(y) - (slope * mean(x))
predictions = intercept + slope*x
SSE = sum((y - predictions)^2)
abline(intercept, slope, col="red")
# c)
# Predict x from y
slope = r *(sd(x) / sd(y))
intercept = mean(x) - (slope * mean(y))
predictions = intercept + slope*x
SSE = sum((x - predictions)^2)
abline(intercept, slope, col="blue")
x<- c(-0.2,-0.9,-0.4,0.6,0.4)
y<- c(0.4,-0.3,-0.3,0.5,1.1)
# a) Pearson's Correlation Coefficient is given by
print("Pearson's Correlation Coefficient is =")
r=cor(x,y)
r
# b)
plot(x,y)
# Predict y from x
slope = r *(sd(y) / sd(x))
intercept = mean(y) - (slope * mean(x))
predictions = intercept + slope*x
SSE = sum((y - predictions)^2)
abline(intercept, slope, col="red")
# c)
# Predict x from y
slope2 = r *(sd(x) / sd(y))
intercept2 = mean(x) - (slope * mean(y))
predictions2 = intercept + slope*y
SSE2 = sum((x - predictions)^2)
abline(intercept, slope, col="blue")
abline(intercept2, slope2, col="blue")
abline(intercept, slope, col="red")
source('C:/Stats/Assignment 11/binorm.R)
source('C:/Stats/Assignment 11/')
source('C:/Stats/Assignment 11/binorm.R')
binorm.scatter(cbind(x, y))
abline(intercept, slope, col="red")
abline(intercept2, slope2, col="blue")
x<- c(-0.2,-0.9,-0.4,0.6,0.4)
y<- c(0.4,-0.3,-0.3,0.5,1.1)
# a) Pearson's Correlation Coefficient is given by
print("Pearson's Correlation Coefficient is =")
r=cor(x,y)
r
# b)
plot(x,y)
# Predict y from x
slope = r *(sd(y) / sd(x))
intercept = mean(y) - (slope * mean(x))
predictions = intercept + slope*x
SSE = sum((y - predictions)^2)
abline(intercept, slope, col="red")
# c)
# Predict x from y
plot(y,x)
slope2 = r *(sd(x) / sd(y))
intercept2 = mean(x) - (slope * mean(y))
predictions2 = intercept + slope*y
SSE2 = sum((x - predictions)^2)
abline(intercept2, slope2, col="blue")
# d) Scatter Plot
source('C:/Stats/Assignment 11/binorm.R')
binorm.scatter(cbind(x, y))
abline(intercept, slope, col="red")
abline(intercept2, slope2, col="blue")
abline(slope2, intercept2, col="blue")
abline(intercept2, slope2 col="blue")
source('C:/Stats/Assignment 12/Question2.R', echo=TRUE)
source('C:/Stats/Assignment 12/Question2.R', echo=TRUE)
source('C:/Stats/Assignment 12/examanxiety.txt')
source('C:/Stats/Assignment 12/examanxiety')
source('C:/Stats/Assignment 12/examanxiety')
source('C:/Stats/Assignment 12/examanxiety.txt')
read.table('C:/Stats/Assignment 12/examanxiety.txt')
ext=read.table('C:/Stats/Assignment 12/examanxiety.txt')
summary(ext)
View(ext)
plot(ext$V3,ext$V4)
plot(ext$V4,ext$V3)
# See also the html file for the regression lecture
batting = read.table("C:/Stats/batting.txt", header=TRUE)
summary(batting)
plot(batting$OB2012, batting$OB2013)
# Model 1: Predict the mean
# for everyone
mean(batting$OB2013)
SST = sum((batting$OB2013 - mean(batting$OB2013))^2)
abline(h=mean(batting$OB2013), col="blue")
# Model 2: Predict using regression
slope = cor(batting$OB2012, batting$OB2013) *
sd(batting$OB2013) / sd(batting$OB2012)
intercept = mean(batting$OB2013) -
slope * mean(batting$OB2012)
predictions = intercept + slope*batting$OB2012
SSE = sum((batting$OB2013 - predictions)^2)
abline(intercept, slope, col="red")
# Ratio
SSE/SST
1 - cor(batting$OB2012, batting$OB2013)^2
# Model 3: Predict 2013 OB = 2012 OB
abline(0, 1, col="orange")
sum((batting$OB2013 - batting$OB2012)^2)
# Inference for the slope
# Test hyp that slope = 0
se = sd(batting$OB2013)/sd(batting$OB2012) *
sqrt((1-cor(batting$OB2012, batting$OB2013)^2)/
(length(batting$OB2012)-2))
T = slope / se
# P-value
2 * (1-pt(T, length(batting$OB2012)-2))
# 95% CI
slope - qt(.975, df=length(batting$OB2012)-2)*se
slope + qt(.975, df=length(batting$OB2012)-2)*se
# F-test
r = cor(batting$OB2012, batting$OB2013)
F = (length(batting$OB2012)-2)*r^2/(1-r^2)
# P-value
1 - pf(F, 1, length(batting$OB2012)-2)
# Prediction
# Robinson Cano:
# 2012: 264 OB, 697 AB
# 2013: ? OB, 681 AB
qqnorm(batting$OB2012)
qqnorm(batting$OB2013)
residuals = batting$OB2013 - predictions
qqnorm(residuals)
# Create two new ratio variables
obp2012 = batting$OB2012 / batting$PA2012
obp2013 = batting$OB2013 / batting$PA2013
qqnorm(obp2012)
qqnorm(obp2013)
plot(obp2012, obp2013)
# Fit regression
slope = cor(obp2012,obp2013)*sd(obp2013)/
sd(obp2012)
intercept = mean(obp2013)-slope*mean(obp2012)
# Check
lm(obp2013 ~ obp2012)
cano2012 = 264/697
# Point prediction for 2013 OBP
slope * cano2012 + intercept
# Point prediction for 2013 OB
(slope * cano2012 + intercept) * 681
# Prediction error
resOBP = obp2013 - (slope*obp2012+intercept)
SSE.OBP = sum(resOBP^2)
pred.error = sqrt(SSE.OBP/(length(obp2012)-2))
pred.error * 681
ext=read.table('C:/Stats/Assignment 12/examanxiety.txt')
summary(ext)
plot(ext$V4,ext$V3)
slope = cor(ext$V4, ext$V3) *
sd(ext$V3) / sd(ext$V4)
slope = cor(ext$V3, ext$V4) *
sd(ext$V3) / sd(ext$V4)
slope = cor(ext$V3, ext$V4) *(sd(ext$V3) / sd(ext$V4))
